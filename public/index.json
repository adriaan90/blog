[{"authors":["admin"],"categories":null,"content":"I teach Mechanical Engineering principles, from using equations of movement to how to design a front suspension for an off-road vehicle using FEA. I am also a researcher, using the material I teach to solve problems found in coal transporting trains and diesel cars. I tinker with any software package I can get a hold of. I quite enjoy using Matlab, SolidWorks, LaTeX and Excel.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I teach Mechanical Engineering principles, from using equations of movement to how to design a front suspension for an off-road vehicle using FEA. I am also a researcher, using the material I teach to solve problems found in coal transporting trains and diesel cars. I tinker with any software package I can get a hold of. I quite enjoy using Matlab, SolidWorks, LaTeX and Excel.","tags":null,"title":"Adriaan van Niekerk","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Table of Contents  Introduction What is web scraping? The data source Coding the web scraper  Get the HTML data   What is BeautifulSoup  Installing BeautifulSoup   Extracting the data  Find table ID Cleaning up the scraped data Creating a dict variable   Compiling the tweet  Growth factor Getting stats for the tweet Constructing the tweet     Introduction As part of my aim to learn Python, I decided to teach myself by completing a bunch of projects created in Python. I started off by creating a Twitter bot, and now I want to expand it to web scraping daily stats of a website and tweeting about it. I am specifically interested in the daily stats of the COVID-19 virus (Coronavirus) and what the daily new cases are as well as the global growth factor. The idea to look at the daily stats of the Coronavirus was sparked after watching a video by 3Blue1Brown where he discusses exponential growth of epidemics.\nRight! Let\u0026rsquo;s get started!\nWhat is web scraping? Web scraping, web harvesting, or web data extraction is used for extracting all kinds of data from websites. Web scraping can be done manually by the user, but web scraping typically includes automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the internet, typically into a database or spreadsheet, for later retrieval or analysis.\nThe data source We are interested in the data contained in a table at Worldometer\u0026rsquo;s website, where it lists all the countries together with their current reported coronavirus cases, new cases for the day, total deaths, new deaths for the day, etc. The idea is to get these daily figures, calculate the global growth factor and create a tweet that can be tweeted daily reporting the stats of:\n United Kingdom and South Africa.  Coding the web scraper Get the HTML data First thing you need to do is to get the HTML data of the site you are interested in, into your Python script. We can do this using the Python library, requests. You can install it using\npip install requests\r Once it is installed you can import it to your Python script:\nimport requests\rURL = 'https://www.worldometers.info/coronavirus/#countries'\rpage = requests(get.URL)\r The code retrieves the HTML data that the server sends back and stores that data in a Python object.\nWe have scraped some HTML from this webpage, but when you look at it, it just seems like a huge mess. There are tons of HTML elements and thousands of attributes scattered around. We can now parse this lengthy code response with Beautiful Soup to make it more accessible and pick out the data that we are interested in.\nWhat is BeautifulSoup Beautiful Soup is a Python library used for pulling data out of HTML and XML files. It provides ways of navigating, searching, and modifying the parse tree of a website. It can save programmers hours or days of work when gatering data.\nInstalling BeautifulSoup If you have not done so already, you need to install BeautifulSoup which is the Python library that you will use to scrape data from the webpage. You can install it using the command prompt:\npip install beautifulsoup4\r Once that is done, we can start using it in our Python script.\nfrom bs4 import BeautifulSoup\rsoup = BeautifulSoup(page.content, 'html.parser\r Extracting the data Find table ID The Beautiful Soup object has been created in our Python script and the HTML data of the website has been scraped off of the page. Next we need to get the data that we are interested in, out of the HTML code. We first find the id attribute of the table by using the inspect fuctionality of the Chrome web browser. You right-click anywhere on the webpage and at the bottom of the dropdown list that appears, you select inspect.\nThe ID of the table is main_table_countries_today. We can use this to focus only on the table in the scraped HTML code:\nresults = soup.find(id='main_table_countries_today')\r  With a website like this, it is possible that the structure of the website can change in the future, and this can include changing the id attribute of the table which can cause an error in your code. You will just have to go back to the webpage and update your python code with the new table ID.\n Cleaning up the scraped data If we go and print out the data contained in results, we get:\n\u0026lt;tr style=\u0026quot;\u0026quot;\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; font-size:15px; text-align:left;\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;mt_a\u0026quot; href=\u0026quot;country/uk/\u0026quot;\u0026gt;UK\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;14,543\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; text-align:right;\u0026quot;\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; text-align:right;\u0026quot;\u0026gt;759 \u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; text-align:right;\u0026quot;\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;135\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;text-align:right;font-weight:bold;\u0026quot;\u0026gt;13,649\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;163\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;214\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;11\u0026lt;/td\u0026gt;\r\u0026lt;td style=\u0026quot;text-align:right;font-size:13px;\u0026quot;\u0026gt;\rJan 30 \u0026lt;/td\u0026gt;\r\u0026lt;/tr\u0026gt;\r The data is the entry for the United Kingdom, but there are still a lot of HTML code that we do not want. All the data entries of the table for the given country is wrapped in the HTML element \u0026lt;td ... \u0026lt;\\td\u0026gt;. We can use that knowledge to further clean up the scraped data:\ncontent = results.find_all('td')\r If we print out the data contained in content we get:\n\u0026lt;td style=\u0026quot;font-weight: bold; font-size:15px; text-align:left;\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;mt_a\u0026quot; href=\u0026quot;country/uk/\u0026quot;\u0026gt;UK\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;14,543\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;font-weight: bold; text-align:right;\u0026quot;\u0026gt;\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;font-weight: bold; text-align:right;\u0026quot;\u0026gt;759 \u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;font-weight: bold; text-align:right;\u0026quot;\u0026gt;\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;135\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;text-align:right;font-weight:bold;\u0026quot;\u0026gt;13,649\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;163\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;214\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;font-weight: bold; text-align:right\u0026quot;\u0026gt;11\u0026lt;/td\u0026gt;, \u0026lt;td style=\u0026quot;text-align:right;font-size:13px;\u0026quot;\u0026gt;\r This is still every chaotic. Luckily all the data that we need, it text and can be extracted from the above as:\nfor data in content:\rprint(data.text.strip())\r This will contain the following data:\nUK\r14,543\r759\r135\r13,649\r163\r214\r11\r Each line above is a column entry for the United Kingdom. Next, this data needs to be entered into a dictionary so that we can use it to contruct a tweet.\nCreating a dict variable Creating seperate lists Before we can create a tweet reporting on the daily stats, we need to save the scraped data in some form that can be used effectively. For this project, all the data will be safed in a Python dictionary. To achieve this we will:\n Save each column in a list and Create a dictionary with all the populated lists.  First, we initialise empty lists for each column:\ncountries = []\rtotal_cases = []\rnew_cases = []\rtotal_deaths = []\rnew_deaths = []\rtotal_recovered = []\ractive_cases = []\rcritical = []\rtotal_per_mil_pop = []\r Then we iterate through the content variable and place each corresponding entry into the correct list. There are 10 columns and as such a new country\u0026rsquo;s data is shown after every 10 iterations:\ni = 1\rfor data in content:\rif i%10 == 1:\rcountries.append(data.text.strip())\rif i%10 == 2:\rtotal_cases.append(data.text.strip())\rif i%10 == 3:\rnew_cases.append(data.text.strip())\rif i%10 == 4:\rtotal_deaths.append(data.text.strip())\rif i%10 == 5:\rnew_deaths.append(data.text.strip())\rif i%10 == 6:\rtotal_recovered.append(data.text.strip())\rif i%10 == 7:\ractive_cases.append(data.text.strip())\rif i%10 == 8:\rcritical.append(data.text.strip())\rif i%10 == 0:\rtotal_per_mil_pop.append(data.text.strip())\ri += 1\r If we assume the first column on the left (Country) can be numbered as 1, then the mathematical operater %10 returns the modulus which corresponds to each column. This can then be used to ensure the correct data is appended to the correct list.\nCombining lists into a dictionary After all the lists have been populated with the data scraped from the webpage, we can combine it into a dictionary:\ncovid19_table = {\r\u0026quot;columns\u0026quot;: column_names,\r\u0026quot;country\u0026quot;: countries,\r\u0026quot;total_cases\u0026quot;: total_cases,\r\u0026quot;new_cases\u0026quot;: new_cases,\r\u0026quot;total_deaths\u0026quot;: total_deaths,\r\u0026quot;new_deaths\u0026quot;: new_deaths,\r\u0026quot;total_recovered\u0026quot;: total_recovered,\r\u0026quot;active_cases\u0026quot;: active_cases,\r\u0026quot;critical\u0026quot;: critical,\r\u0026quot;total_1M_ pop\u0026quot;: total_per_mil_pop}\r The column_names have been generated seperately:\ncolumn_names = [\u0026quot;Country\u0026quot;, \u0026quot;Total Cases\u0026quot;, \u0026quot;New Cases\u0026quot;, \u0026quot;Total Deaths\u0026quot;, \u0026quot;New Deaths\u0026quot;, \u0026quot;Total Recovered\u0026quot;, \u0026quot;Active Cases\u0026quot;, \u0026quot;Serious/Critical\u0026quot;, \u0026quot;Total Cases/1M pop\u0026quot;]\r We are now ready to start using this data to contruct a tweet that will be sent out daily.\nCompiling the tweet Growth factor In order to calculate the growth factor of the coronavirus, the following equation can be used:\n$$Gf = \\frac{N_i}{N_{i-1}}$$\nwhere $N_i$ is the amount of new cases for today and $N_{i-1}$ refers to the amount of new cases for the previous day.\nA .csv file will be used to store each day\u0026rsquo;s stats so that a new growth factor can be calculated each day:\nDate,Total_cases,New_cases,Growth_factor\r2020-03-11,\u0026quot;126,007\u0026quot;,\u0026quot;7,059\u0026quot;,0.0\r2020-03-12,\u0026quot;134,098\u0026quot;,\u0026quot;7,899\u0026quot;,1.12\r2020-03-13,\u0026quot;145,336\u0026quot;,\u0026quot;10,759\u0026quot;,1.36\r A Python libary pandas will be used to read from, and write to the .csv file. You can install pandas using:\npip install pandas\r The following was added to the python script to read the current content of the .csv file and put in a dictionary new_table:\nimport pandas\rdf = pandas.read_csv('Total.csv', parse_dates = [\u0026quot;Date\u0026quot;], dayfirst = True)\rnew_table = df.to_dict()\r The current date is added to the new_table dictionary as it is the first entry required for the .csv file:\ntoday = str(date.today())\rnew_table[\u0026quot;Date\u0026quot;][len(new_table[\u0026quot;Date\u0026quot;])] = today\r Now we calculate the growth factor. We first search for the position in the country label of our covid19_table dictionary for the total numbers for the whole world for the current day. This is under the *country * entry Total:.\nsearch_position = covid19_table[\u0026quot;country\u0026quot;].index(\u0026quot;Total:\u0026quot;)\r Next, we retrieve yesterday\u0026rsquo;s data from the new_table dictionary which was created from the .csv file:\ngrowth_yesterday = new_table[\u0026quot;New_cases\u0026quot;][len(new_table[\u0026quot;New_cases\u0026quot;])-1]\rif type(growth_yesterday) == str:\rgrowth_yesterday = new_table[\u0026quot;New_cases\u0026quot;][len(new_table[\u0026quot;New_cases\u0026quot;])-1].replace(',','')\r The amount of new cases globally is retrieved from the covid19_table that we created from our scraped data using the search_position which indicates where the corresponding data is in the dictionary.\ngrowth_today = covid19_table[\u0026quot;new_cases\u0026quot;][search_position].replace(',','')\r The growth factor can now be calculated:\nGf = round(float(growth_today)/float(growth_yesterday),2)\r The data needed for the current day\u0026rsquo;s entry for the .csv file is then added to the new_table dictionary:\nnew_table[\u0026quot;Total_cases\u0026quot;][len(new_table[\u0026quot;Total_cases\u0026quot;])] = covid19_table[\u0026quot;total_cases\u0026quot;][search_position]\rnew_table[\u0026quot;New_cases\u0026quot;][len(new_table[\u0026quot;New_cases\u0026quot;])] = covid19_table[\u0026quot;new_cases\u0026quot;][search_position]\rnew_table[\u0026quot;Growth_factor\u0026quot;][len(new_table[\u0026quot;Growth_factor\u0026quot;])] = str(Gf)\r Lastly, we convert the new_table dictionary back to a .csv file that can be used the next day again:\ndf = pandas.DataFrame.from_dict(new_table)\rdf.to_csv(\u0026quot;Total.csv\u0026quot;,index=False)\r Getting stats for the tweet We create a dictionary that can be returned when the method is called with all the necessary data required for the tweet:\nposition_UK = covid19_table[\u0026quot;country\u0026quot;].index(\u0026quot;UK\u0026quot;)\rposition_RSA = covid19_table[\u0026quot;country\u0026quot;].index(\u0026quot;South Africa\u0026quot;)\rnew_UK = covid19_table[\u0026quot;new_cases\u0026quot;][position_UK]\rnew_RSA = covid19_table[\u0026quot;new_cases\u0026quot;][position_RSA]\rnew_total = covid19_table[\u0026quot;new_cases\u0026quot;][search_position].replace(',','')\rtotal_UK = covid19_table[\u0026quot;total_cases\u0026quot;][position_UK]\rtotal_RSA = covid19_table[\u0026quot;total_cases\u0026quot;][position_RSA]\rtotal_total = covid19_table[\u0026quot;total_cases\u0026quot;][search_position].replace(',','')\rtweet_data = {\r\u0026quot;UK\u0026quot;: {\r\u0026quot;Total\u0026quot;: total_UK,\r\u0026quot;New\u0026quot;: new_UK\r},\r\u0026quot;RSA\u0026quot;: {\r\u0026quot;Total\u0026quot;:total_RSA,\r\u0026quot;New\u0026quot;:new_RSA\r},\r\u0026quot;Total\u0026quot;: {\r\u0026quot;Total\u0026quot;: total_total,\r\u0026quot;New\u0026quot;: new_total\r},\r\u0026quot;Gf\u0026quot;: Gf\r}\r The tweet_data dictionary can now be used to construct the tweet.\nConstructing the tweet The best way to create a multi-line tweet is to use a .txt file that can be fed to the twitter API. The content of the .txt file for the tweet was written together with the data in tweet_data:\nwith open('tweet.txt', 'w',encoding='utf-8') as f:\rf.write('#COVID19 stats '+str(date.today())+':\\n\\\r\\nTotal cases for:\\nUK: '+str(tweet_data[\u0026quot;UK\u0026quot;][\u0026quot;Total\u0026quot;])+' ('+str(tweet_data[\u0026quot;UK\u0026quot;][\u0026quot;New\u0026quot;])+')'+'\\\r\\nRSA: '+str(tweet_data[\u0026quot;RSA\u0026quot;][\u0026quot;Total\u0026quot;])+' ('+str(tweet_data[\u0026quot;RSA\u0026quot;][\u0026quot;New\u0026quot;])+')'+'\\\r\\nOverall: '+str(tweet_data[\u0026quot;Total\u0026quot;][\u0026quot;Total\u0026quot;])+' (+'+str(tweet_data[\u0026quot;Total\u0026quot;][\u0026quot;New\u0026quot;])+')'+'\\\r\\n\\nGrowth factor: '+str(tweet_data[\u0026quot;Gf\u0026quot;])+'\\\r\\n\\n#CoronaVirusSA \\n#CoronaVirusUK')\r The last thing left is to tweet it!\nwith open('tweet.txt','r') as f:\rapi.update_status(f.read())\r #COVID19 stats 2020-03-27:\nTotal cases for:\nUK: 14,543 (+2,885) RSA: 1,170 (+243) Overall: 590628 (+58818) Growth factor: 1.0 #CoronaVirusSA #CoronaVirusUK\n\u0026mdash; COVID-19 stats tracker (@nog_nuus) March 27, 2020 ","date":1585353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585353600,"objectID":"eaa743ed72bf9679548d813ee82cd471","permalink":"/project/web-scrape-coronavirus/","publishdate":"2020-03-28T00:00:00Z","relpermalink":"/project/web-scrape-coronavirus/","section":"project","summary":"How to web scrape data from a website.","tags":["Python","Twitter","COVID-19","Coronavirus"],"title":"Web scraping for daily COVID-19 stats","type":"project"},{"authors":null,"categories":null,"content":"As part of my aim to learn Python, I decided to teach myself by completing a bunch of projects created in Python, and a Twitter bot is my first project!\nThe purpose of this article is two-fold:\n Helping you to create a Twitter bot of your own. Record the process for when I return to this project in the future.  Right! Let\u0026rsquo;s get started!\nSome background watching I came across a video by CS Dojo How To Create A Twitter Bot With Python. Do check it out! I also created a video showing the process for those that rather prefer watching how it is done vs. reading about it.\n\rGetting started with Twitter API The first thing you need to do is create an account at Twitter Developer. Make sure you are logged into the Twitter account that you want to use to create the Twitter bot. Create your account and add an app that will generate your authorisation keys that you need to connect to the Twitter API. These keys will be used in the Python script to connect to the API.\nConnect to the Twitter API with Python The first thing you need to do is to install tweepy which is the Python library that you will use to connect to the Twitter API. You can install is using the command prompt:\npip install tweepy\r Once that is done, you can get started with setting up your twitter_bot.py script.\nimport tweepy\rCONSUMER_KEY = 'YOUR_CONSUMER_KEY'\rCONSUMER_SECRET = 'YOUR_CONSUMER_SECRET_KEY'\rACCESS_KEY = 'YOUR_ACCESS_KEY'\rACCESS_SECRET = 'YOUR_ACCESS_SECRET_KEY'\rauth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\rauth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\rapi = tweepy.API(auth, wait_on_rate_limit=True)\r You can copy the four key values from your created app in your Twitter Developer account.\n If you plan to commit your code to an online repository, do not add your access keys to your code directly, as others will be able to see it.\n The last three lines of the code sets up the API so that you can use it for all kinds of cool stuff. Make sure you go have a look at the Tweepy documentation to find out what the tweepy library can do.\nFav and retweet! For this project, I want to create a bot that will favourite a tweet and retweet it when your twitter account is mentioned.\nScan for any tweets mentioning the account First we need to collect all the tweets that mention the account. Tweepy enables you to collect the 20 latest tweets mentioning your account.\nIt is necessary to specify the last tweet ID that you interacted with to ensure that you don\u0026rsquo;t retweet or favourite any tweet you already have.\nWe can store the ID of the last tweet that we enteracted with in a .txt file.\ndef retrieve_last_seen_id(file_name):\rf_read = open(file_name, 'r')\rlast_seen_id = int(f_read.read().strip())\rf_read.close()\rreturn last_seen_id\r The function opens up the textfile and reads the tweet ID and returns it as an integer in the variable last_seen_id.\nNext we retrieve all the tweets mentioning your twitter account after the last tweet ID specified in your textfile.\nmentions = api.mentions_timeline(last_seen_id, tweet_mode = 'extended')\r We loop through all the tweet IDs in the mentions variable. The loop starts at the end of the list, which will be the oldest tweet that we haven\u0026rsquo;t interact with yet.\nfor mention in reversed(mentions):\rif not mention:\rreturn\rlast_fav_tweet = mention.id\rstore_last_seen_id(last_fav_tweet,FILE_NAME_FAV)\r The tweet ID of the last interacted tweet is updated in the text file, using the function:\ndef store_last_seen_id(last_seen_id,file_name):\rf_write = open(file_name, 'w')\rf_write.write(str(last_seen_id))\rf_write.close()\rreturn\r The tweet ID stored in the mention.id is then favourited and retweeted with the commands:\napi.create_favorite(mention.id)\rapi.retweet(mention.id)\r Deploying the bot Now that you have set up the bot it needs to run continuously to check for tweets mentioning your account. This can be done by putting the function in an infinite loop:\nwhile True:\rfav_tweets()\rtime.sleep(15)\r The code time.sleep() pauses the while loop for a period specified here as 15 seconds. Make sure to add\nimport time\r to your script to enable this functionality.\nUsing Heroku The problem with running the bot locally on your computer is that your command prompt window will be occupied running the bot, so you won\u0026rsquo;t be able to use it.\nThe best solution is to deploy it on a server, and Heroku is a free service for just that purpose! The best place to start is to follow Heroku\u0026rsquo;s step-by-step guide for deploying using Python.\nYou will need to sign up for an account and then install the Heroku CLI to your computer. You can either link your Heroku account to your Github repository where your bot has been uploaded, or you can upload it to a Heroku repository.\nWe will use the Heroku repository. Once you have installed the Heroku CLI to your computer, login using the command prompt:\n$ heroku login\r Create a Heroku app The next step is to initiate a git repository on your local machine using:\n$ git init\r$ cd the-name-of-your-bot\r  You can also clone the getting-started-with-python repository that Heroku provides. It has all the additional documents already created that you need. All you need to do is copy those files over to your project\u0026rsquo;s folder.\n After a git repository has been initialised, you need to commit all your changes and then create an app on Heroku, which prepares Heroku to receive your source code:\n$ git add .\r$ git commit -m \u0026quot;initial commit\u0026quot;\r$ heroku create\r$ git push heroku master\r Last few things So the last problem is that you need to get your API authorisation keys to the Heroku repository without making it public knowledge.\nBefore you push your repository to Heroku, make the following changes to your code:\nimport os\rfrom os import environ\rCONSUMER_KEY = environ['YOUR_CONSUMER_KEY']\rCONSUMER_SECRET = environ['YOUR_CONSUMER_SECRET']\rACCESS_KEY = environ['YOUR_ACCESS_KEY']\rACCESS_SECRET = environ['YOUR_ACCESS_SECRET']\r The above code makes sure that you do not need to commit your Twitter API keys. After you have edited your code, you need to add your API keys to your Heroku account by going to your Heroku app, going to settings and editing the Config Vars.\nAlso, there are a few more files that you need to upload to Heroku before it will be functional. This include:\n Procfile Requirements.txt Runtime.txt   This is only necessary if you haven\u0026rsquo;t already copied them over from the repository getting-started-with-python that you cloned.\n Procfile Heroku apps include a Procfile that specifies the commands that are executed by the app on startup. You need to use a Procfile to declare that this is a worker:\nworker: python your_bot_name.py\r Requirements.txt The requirements file lists the versions of all the programs that your bot will use:\ncertifi==2019.11.28\rchardet==3.0.4\ridna==2.9\roauthlib==3.1.0\rPySocks==1.7.1\rrequests==2.23.0\rrequests-oauthlib==1.3.0\rsix==1.14.0\rtweepy==3.6.0\rurllib3==1.25.8\r Runtime file This file specifies the version of Python that Heroku needs to use. At the time of writing this, Tweepy does not support Python 3.7, so you need to specify an older version.\npython-3.6.9\r ","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583971200,"objectID":"d680242b3152ab2c39d5d665e163915b","permalink":"/project/twitter-bot-project/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/project/twitter-bot-project/","section":"project","summary":"How to create a Twitter bot","tags":["Python","Twitter"],"title":"Create a Twitter bot","type":"project"},{"authors":[],"categories":[],"content":"\r\rNOx emissions have become a health concern in cities with calls to reduce and ban vehicles with CI engines from entering the city. As such, our aim is to reduce NOx emissions being produced by the CI engine, and Low Temperature Combustion (LTC) is seen as a viable option.\nLTC is a broad term used generally for combustion techniques where the overall peak combustion temperature is reduced. This is beneficial as it reduces the formation of NOx exhaust gasses. LTC techniques include HCCI, PCCI and RCCI. Although NOx emissions is reduced as a result of lower temperatures, other emissions such as CO emissions and HC emissions can increase due to incomplete combustion.\nThere is thus a balance that needs to be optimised to ensure an overall reduction of all emissions. This research focussed on achieving emissions reduction by optimising EGR and the engine’s fuel delivery.\nFor this research we looked at four parameters that can be used to achieve LTC and ultimately reduce NOx and CO emissions. NOx emissions is reduced by reducing the combustion temperature and CO emissions are reduced by increasing the homogeneity of the air fuel mixture. The table shown in the slide shows the effects on the combustion temperature and the homogeneity of the air fuel mixture as reported in the literature.\nIf we increase pilot injection duration, then the homogeneity of the charge increases as more fuel is being introduced in the pilot injection and thus more of the fuel can mix with the air before combustion occurs. Premixed combustion also increases as a result of this, ultimately increasing the combustion temperature.\nIf we advance the pilot injection start of injection (SOI), then the homogeneity of the charge is increased, as there is more time for the fuel to mix with the air before combustion occurs. Increased homogeneity also increases the premixed combustion, which increases the combustion temperature.\nIf we advance the main injection SOI, then the time for the fuel to mix with the air is decreased, which reduces the homogeneity of the charge as well as the combustion temperature.\nIf we increase the Exhaust gas recirculation (EGR) percentage, then the homogeneity of the charge is increased, as more EGR increases the ignition delay and gives the fuel more time to mix with the air. It also reduces the combustion temperature as more inert gasses are introduced into the inlet charge, which absorbs a lot of the heat.\nFrom this table, it can be seen that the four parameters have different effects on combustion temperature and charge homogeneity. It is thus necessary to determine which parameter has a significant effect on emissions formation and which parameter has a lesser effect.\nThe Design of Experiment (DoE) statistical tool was used to determine the effect of each parameter on the formation of engine emissions and if it is significant or not. A DoE was also used to determine the impact of each parameter as well as determine an optimised point that resulted in overall reduced emissions.\nNext we need to consider the drive cycle that will be used in the simulation. When we look at how past research has generated experimental emissions data, the majority of research found have used steady state engine operating points in their test methodology. The results from steady state experimentation cannot accurately represent real life scenarios. A transient drive cycle is needed to generate results that are comparable to real life. The WLTP was used in this research. It replaced the NEDC that has been used in the past to test the new vehicle entering the market.\nI have created this figure to further illustrate the benefits of using the WLTP for real world emissions investigation. The graph shows the WLTP, in grey circles, as a function of engine speed and BMEP. The red crosses indicate the steady state points used by past research to investigate engine emissions. When looking at the graph, clear gaps are evident in the engine operating map that is not covered by the research considered. The use of a transient drive cycle is thus appropriate if the results needed to be comparable to a real world scenario.\nMethodology\rAn engine simulation was used to investigate the effects of varying the different engine operating parameters on engine emissions. A 2.4 L turbocharged CI engine was simulated. The simulation’s combustion model was validated using in-cylinder pressure data and emissions data was used to validate its emissions models.\nThe Wiebe combustion model was used in this research. Linear regression models was generated for the start of combustion crank angle degree (CAD), premixed fuel mass fraction burned and Wiebe exponent by using the cylinder pressure data. This is necessary as we are simulating a transient drive cycle as well as changing engine parameters that influence combustion.\rThe emission models were validated using exhaust gas analyser experimental data. The engine simulation can only simulate CO emissions and NOx emissions.\nAfter the models were calibrated, it was compared to experimental data to check its accuracy. Here two cylinder pressure graphs are shown; one at 25% load and 1500rpm and the other at 75% load and 3000rpm. The experimental data is shown in a solid line and the simulated model is shown in dashed lines. As can be seen, the simulated results correlate well with the experimental data.\nSame can be said of the simulated emissions. The simulated emission results for the CO emissions and NOx emissions correlate well with the experimental data. We thus have confidence in our simulation and can now move on to setting up the DoE.\nWhen setting up the factorial design, a 2^4 factorial design was chosen as there are 4 parameters that will be investigated. These are EGR percentage, pilot injection duration and main and pilot injection SOI. The test engine is using an aftermarket ECU, which have operating maps loaded onto it by default. These operating maps are used as the starting point for the DoE. The EGR percentage map is in the form of an island with maximum EGR at approximately 2500 rpm and 10 % throttle position. Here is an example of an EGR map with 47% as the maximum percentage. The value as given by the DoE will always be the maximum value and the maps will be scaled according to the maximum value.\nShown here is a table with the low and high values of the first factorial design.\rThe EGR percentage has a low and high value of 0 % and 10 %, pilot injection and main injection SOI is advanced by one CAD and retarded by 1 CAD. The pilot injection duration is decreased by 100 μs and increased by 100 μs. Similar tables were created for the second and third factorial designs based on the results from the previous factorial design.\nIn order to determine the configuration that will reduce emissions the most, we opted to follow the path of greatest emission reduction using multiple factorial designs. After each factorial design, The desirability function was used to determine the best configuration of the parameters under investigation. This then was used to set up the next factorial design. This can be explained in the figure shown.\nThe figure shows the three factorial designs for two parameters, pilot injection SOI and EGR percentage. As can be seen for the first factorial design, the EGR is varied from 0 % to 10 % and the pilot injection SOI is advanced and retarded by 1 CAD. Once the first factorial design is completed, the desirability function is used to determine which configuration reduces the emissions the most. In this case it is an EGR percentage of 10 % and by retarding the pilot injection SOI by 1 CAD. The low and high values of the second factorial design can now be determined with the use of the two equations shown on the slide.\nAs a maximum desirability (Di) is achieved at 10 % EGR, the second factorial design’s low value becomes 10 % and the high level value for the second factorial design becomes 25 %. For the pilot injection SOI, the second factorial design’s low value is set to retard the map by 1 CAD and the high values is set to retard the operating map by 4 CADs.\nOnce the second factorial design is finished, the desirability function is used again to determine which configuration results in the reducing the emissions the most. This results in a EGR percentage of 25 % and retarding the pilot injection SOI by 2 CADs. As such the low and high values for the EGR percentage for the third factorial design is calculated as 25 % and 47.5%. The pilot injection SOI low and high values for the third factorial design stays at retarding the maps with 1 CAD and 4 CADs.\n\rResults and discussion\rShown on this slide is the desirability function plot for the third factorial design that was simulated. As can be seen, by the third factorial design, the start of injection for the pilot and main injection and the injection duration of the pilot injection does not significantly influence the emissions. This can be seen by the almost horizontal lines of the graph.\nThe EGR percentage has a significant effect on the NOx emissions and CO emissions as the graph lines have a steep gradient. The desirability function plot at the top in the form of a half circle indicate that the maximum desirability value will be reached for an EGR percentage at approximately 36 %.\nWhen we put all the factorial designs’ results on one graph, we can get a better overall picture.\rFor the second factorial design, we optimised towards a maximum of CO emissions as per the Euro 4 limits. This resulted in a reduction of approximately 20% of NOx when we use a maximum of 12% EGR.\nTo further investigate LTC, for the third factorial design, we opted to continue to increase the EGR percentage to 47.5%. This resulted in a reduction in NOx of 85% to 0.55g/km where the Euro 4 limit is 0.25g/km. The CO emissions greatly increased as a result of the EGR percentage increasing, to 22.58g/km.\nNext we wanted to see if we achieved LTC. This graph shows the peak temperature for combustion with no LTC techniques used as well as for the combustion temperature for the third factorial design. The difference in peak temperature between the two graphs is approximately 100 °K.\n\rConclusions\r\rNOx emissions were reduced by approximately 85% with an EGR percentage of 47.5 %, retarding the pilot injection and main injection SOI by 1 CAD and increasing the pilot injection duration by 200 μs.\rNOx emissions reduced by approximately 20 % with the use of 12 % EGR without exceeding the Euro 4 CO emissions limit.\rLow temperature combustion was achieved\rThe method of using DoE to minimise engine out emissions was successful\r\r\rLimitations\r\rThe sample size of the experimental data is modest. By using more experimental data, more robust regression models can be constructed.\rA blind transient comparison between simulation and experimental results would be beneficial and increase our confidence in our engine simulation.\rFollowing the path of greatest emission reduction, was successful, but it can result in finding a local minimum, where we want to determine the global minimum.\rThe DoE can be improved by investigate the whole operating map of the engine to ensure that we will be able to determine the global minimum.\r\r\r","date":1566777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566777600,"objectID":"906bf531dd72ae63757a9a7529588244","permalink":"/post/jsae_conference/","publishdate":"2019-08-26T00:00:00Z","relpermalink":"/post/jsae_conference/","section":"post","summary":"NOx emissions have become a health concern in cities with calls to reduce and ban vehicles with CI engines from entering the city. As such, our aim is to reduce NOx emissions being produced by the CI engine, and Low Temperature Combustion (LTC) is seen as a viable option.\nLTC is a broad term used generally for combustion techniques where the overall peak combustion temperature is reduced. This is beneficial as it reduces the formation of NOx exhaust gasses.","tags":[],"title":"JSAE/SAE conference in Kyoto, Japan","type":"post"},{"authors":["Adriaan van Niekerk"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Adriaan van Niekerk","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Adriaan van Niekerk","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]